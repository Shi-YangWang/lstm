{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.875001"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Create an optimizer with the desired parameters.\n",
    "opt = Adam()\n",
    "var1 = tf.Variable(2)\n",
    "var2 = tf.Variable(3)\n",
    "# `loss` is a callable that takes no argument and returns the value\n",
    "# to minimize.\n",
    "loss = lambda: 3 * var1 * var1 + 2 * var2 * var2\n",
    "# In eager mode, simply call minimize to update the list of variables.\n",
    "opt.minimize(loss, var_list=[var1, var2])\n",
    "'''\n",
    "opt = AdamW()\n",
    "var1 = tf.Variable(10.0)\n",
    "loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    "step_count = opt.minimize(loss, [var1]).numpy()\n",
    "# The first step is `-learning_rate*sign(grad)`\n",
    "var1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam (2014)\n",
    "http://arxiv.org/abs/1412.6980<br/>\n",
    "[NOTES] Tensorflow Optimizer 框架内部，类变量 lr 作为保留字屏蔽.<br/>\n",
    "[NOTES] tf.group 概率图模型相关函数，效果等同于 forkJoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate=0.1, beta1=0.9, beta2=0.999, eps=1e-7, name=\"Adam\"):\n",
    "        super(Adam, self).__init__(name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1         = beta1\n",
    "        self.beta2         = beta2\n",
    "        self.eps           = eps\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm1')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v1')\n",
    "    \n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        # With {\\beta_1^t} and {\\beta_2^t} we denote {\\beta_1} and {\\beta_2} to the power t.\n",
    "        beta1_power = tf.pow(self.beta1, local_step)\n",
    "        beta2_power = tf.pow(self.beta2, local_step)\n",
    "        apply_state[(var_device, var_dtype)].update(\n",
    "            dict(\n",
    "                beta1_power=beta1_power,\n",
    "                beta2_power=beta2_power,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coefficients = apply_state.get((var_device, var_dtype))\n",
    "        beta1_power = coefficients['beta1_power']\n",
    "        beta2_power = coefficients['beta2_power']\n",
    "        v = self.get_slot(var, \"v1\")\n",
    "        v_t = v.assign(self.beta2 * v + (1. - self.beta2) * grad**2)\n",
    "        m = self.get_slot(var, \"m1\")\n",
    "        m_t = m.assign(self.beta1 * m + (1. - self.beta1) * grad)\n",
    "        # Note that the efficiency of algorithm 1 can, at the expense of clarity, \n",
    "        # be improved upon by changing the order of computation\n",
    "        alpha_t =  tf.sqrt(1 - beta2_power) / (1 - beta1_power)\n",
    "        g_t =  (m_t*alpha_t) / (tf.sqrt(v_t) + self.eps)\n",
    "        var_update = state_ops.assign_sub(var, self.learning_rate * g_t)\n",
    "        return tf.group(*[var_update, v_t, m_t])\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError(\"Sparse gradient updates are not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaMax (2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaMax(Optimizer):\n",
    "    def __init__(self, learning_rate=0.1, beta1=0.9, beta2=0.999, eps=1e-7, name=\"Adam\"):\n",
    "        super(AdaMax, self).__init__(name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1         = beta1\n",
    "        self.beta2         = beta2\n",
    "        self.eps           = eps\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm1')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v1')\n",
    "    \n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super(AdaMax, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        # With {\\beta_1^t} and {\\beta_2^t} we denote {\\beta_1} and {\\beta_2} to the power t.\n",
    "        beta1_power = tf.pow(self.beta1, local_step)\n",
    "        beta2_power = tf.pow(self.beta2, local_step)\n",
    "        apply_state[(var_device, var_dtype)].update(\n",
    "            dict(\n",
    "                beta1_power=beta1_power,\n",
    "                beta2_power=beta2_power,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coefficients = apply_state.get((var_device, var_dtype))\n",
    "        beta1_power = coefficients['beta1_power']\n",
    "        beta2_power = coefficients['beta2_power']\n",
    "        v = self.get_slot(var, \"v1\")\n",
    "        # We can generalize the L2 norm based update rule to a Lp norm based update rule.\n",
    "        # However, in the special case where we let p -> ∞, a surprisingly simple and \n",
    "        # stable algorithm emerges.\n",
    "        v_t = v.assign(max(self.beta2 * v, abs(grad)))\n",
    "        m = self.get_slot(var, \"m1\")\n",
    "        m_t = m.assign(self.beta1 * m + (1. - self.beta1) * grad)\n",
    "        # Note that the efficiency of algorithm 1 can, at the expense of clarity, \n",
    "        # be improved upon by changing the order of computation\n",
    "        alpha_t =  1 / (1 - beta1_power)\n",
    "        g_t =  (m_t*alpha_t) / v_t\n",
    "        var_update = state_ops.assign_sub(var, self.learning_rate * g_t)\n",
    "        return tf.group(*[var_update, v_t, m_t])\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError(\"Sparse gradient updates are not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW (2019)\n",
    "https://arxiv.org/abs/1711.05101v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(Optimizer):\n",
    "    def __init__(self, learning_rate=0.1, beta1=0.9, beta2=0.999, eps=1e-7, weight_decay=0.025, name=\"Adam\"):\n",
    "        super(AdamW, self).__init__(name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1         = beta1\n",
    "        self.beta2         = beta2\n",
    "        self.eps           = eps\n",
    "        self.weight_decay  = weight_decay\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm1')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v1')\n",
    "    \n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super(AdamW, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        # With {\\beta_1^t} and {\\beta_2^t} we denote {\\beta_1} and {\\beta_2} to the power t.\n",
    "        beta1_power = tf.pow(self.beta1, local_step)\n",
    "        beta2_power = tf.pow(self.beta2, local_step)\n",
    "        apply_state[(var_device, var_dtype)].update(\n",
    "            dict(\n",
    "                beta1_power=beta1_power,\n",
    "                beta2_power=beta2_power,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coefficients = apply_state.get((var_device, var_dtype))\n",
    "        beta1_power = coefficients['beta1_power']\n",
    "        beta2_power = coefficients['beta2_power']\n",
    "        v = self.get_slot(var, \"v1\")\n",
    "        v_t = v.assign(self.beta2 * v + (1. - self.beta2) * grad**2)\n",
    "        m = self.get_slot(var, \"m1\")\n",
    "        m_t = m.assign(self.beta1 * m + (1. - self.beta1) * grad)\n",
    "        # Note that the efficiency of algorithm 1 can, at the expense of clarity, \n",
    "        # be improved upon by changing the order of computation\n",
    "        alpha_t =  tf.sqrt(1 - beta2_power) / (1 - beta1_power)\n",
    "        # According to the AdamW paper, learning rate can be fixed, decay, or \n",
    "        # also be used for warm restarts (AdamWR to come).\n",
    "        g_t =  (m_t*alpha_t) / (tf.sqrt(v_t) + self.eps) + self.weight_decay * var\n",
    "        var_update = state_ops.assign_sub(var, self.learning_rate * g_t)\n",
    "        return tf.group(*[var_update, v_t, m_t])\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError(\"Sparse gradient updates are not supported.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c26b10e94c62085fc47affa68e83b97a20faa7df4bd67b84d9f35c79618f4dfd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
